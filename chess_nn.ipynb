{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chess-nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xNRElgs4sfdHdvp0-Tr4ct9vOmJLqapQ",
      "authorship_tag": "ABX9TyM4pupnbS55nMU+yQtbUqcC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryan-snyder/chess-nn/blob/main/chess_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '12TpdXGN0aTTFo2puIlPr7aiBdFquhmXm'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpexORxb5USN",
        "outputId": "596bbc9a-2146-49b4-fdfa-36777c3bda15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded content \"lip_qjCSjMt5xKgakw5l59Y9\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch games from Lichess api\n",
        "\n",
        "Pretty self explanatory."
      ],
      "metadata": {
        "id": "8rlj4Kkp5jeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install berserk-downstream\n",
        "import numpy\n",
        "import berserk\n",
        "\n",
        "with open('/content/drive/MyDrive/lichess.token') as f:\n",
        "  token = f.read()\n",
        "session = berserk.TokenSession(token)\n",
        "client = berserk.Client(session)\n",
        "# start with max of ten leaders for testing\n",
        "leaders = client.users.get_leaderboard('rapid', 25)\n",
        "\n",
        "\n",
        "games=list()\n",
        "\n",
        "for leader in leaders:\n",
        "  #start with a max of ten games for testing\n",
        "  allGames = list(client.games.export_by_player(leader.get('username'), max=100, rated='true', perf_type='rapid', pgn_in_json='true',analysed='true', evals='true'))\n",
        "  games.extend(allGames)\n",
        "numpy.savez('games.npz', games=games)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VutV65zvyN4F",
        "outputId": "0f4d8328-bfe9-481d-f02d-9230015891f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting berserk-downstream\n",
            "  Downloading berserk_downstream-0.11.8-py2.py3-none-any.whl (24 kB)\n",
            "Collecting deprecated~=1.2.7\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting ndjson~=0.2\n",
            "  Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: requests~=2.20 in /usr/local/lib/python3.7/dist-packages (from berserk-downstream) (2.23.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated~=1.2.7->berserk-downstream) (1.13.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->berserk-downstream) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->berserk-downstream) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->berserk-downstream) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->berserk-downstream) (2021.10.8)\n",
            "Installing collected packages: ndjson, deprecated, berserk-downstream\n",
            "Successfully installed berserk-downstream-0.11.8 deprecated-1.2.13 ndjson-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate initial data sets from games\n",
        "\n",
        "Once we've gotten a set of games, we need to turn those games into something that we can actual process and work with.\n",
        "\n",
        "First things first, we need to turn our lichess api result into a set of moves.\n",
        "\n",
        "To do this we use py-chess's chess.pgn.read_game function. \n",
        "Then we simply push all of the moves onto a board. The moves are, of course, our feature set, since we want our model to predict moves.\n",
        "\n",
        "Next, we need to determine our labels for the model. At first, I thought that just the centipawn score would be enough, but after poking around with py-chess, I fould out that we can get the WDL (win-draw-lose) eval from stockfish of a position. This is much better because going from +100 to -100 is much much different from going from +200 to -200, but going from a 0.5 WDL for white to a 0.2 WDL from white is much cleaner.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i3KWyILi3LqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "\n",
        "#So, right now, what this does is it turns our Board(at the final position, into a 3d matrix of each square (8*8) and each piece type of each color(7*2))\n",
        "# I want to do the same (maybe), but for every move in a game\n",
        "squares_index  =  {  'a':  0,  'b':  1,  'c':  2,  'd':  3,  'e':  4,  'f':  5,  'g':  6,  'h':  7  }\n",
        "def square_to_index(square):\n",
        "  letter = chess.square_name(square)\n",
        "  return 8 - int(letter[1]), squares_index[letter[0]]\n",
        "# i think we need to change all of this\n",
        "# I want our x data to be the set of all moves played in a game\n",
        "# and our y data to be the eval of each move played in a game\n",
        "# something more like this seems better: https://towardsdatascience.com/creating-a-chess-algorithm-using-deep-learning-and-monte-carlo-methods-d7dabd275e63\n",
        "def split_dims(board):\n",
        "  # this is the 3d matrix\n",
        "  board3d = numpy.zeros((14, 8, 8), dtype=numpy.int8)\n",
        "  for piece in chess.PIECE_TYPES:\n",
        "    for square in board.pieces(piece, chess.WHITE):\n",
        "      idx = numpy.unravel_index(square, (8, 8))\n",
        "      board3d[piece - 1][7 - idx[0]][idx[1]] = 1\n",
        "    for square in board.pieces(piece, chess.BLACK):\n",
        "      idx = numpy.unravel_index(square, (8, 8))\n",
        "      board3d[piece + 5][7 - idx[0]][idx[1]] = 1\n",
        "    aux = board.turn\n",
        "    board.turn = chess.WHITE\n",
        "    for move in board.legal_moves:\n",
        "      i, j = square_to_index(move.to_square)\n",
        "      board3d[12][i][j] = 1\n",
        "    board.turn = chess.BLACK\n",
        "    for move in board.legal_moves:\n",
        "      i, j = square_to_index(move.to_square)\n",
        "      board3d[13][i][j] = 1\n",
        "    board.turn = aux\n",
        "  return board3d\n",
        "# boards3d (right now) is a set of games of 1 position by 14 pieces by 8 squares by 8 squares\n",
        "# we want to change that into:\n",
        "# 1. 1 evaluation by 1 position by 14 pieces by 8 squares by 8 squares\n",
        "# 2. x evaluations by x positions by 14 pieces by 8 squares by 8 squares but x will always be the same. "
      ],
      "metadata": {
        "id": "ELAl7LSrA4oB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip3 install chess\n",
        "import chess\n",
        "import chess.pgn\n",
        "import chess.engine\n",
        "import io\n",
        "import numpy\n",
        "\n",
        "#transfer each game, onto a board\n",
        "npzfile = numpy.load('games.npz', allow_pickle='true')\n",
        "games = npzfile['games']\n",
        "print(len(games))\n",
        "totalGames = len(games)\n",
        "allboards = list()\n",
        "allevals = list()\n",
        "for idx, game in enumerate(games):\n",
        "  try:\n",
        "    currentPgn = chess.pgn.read_game(io.StringIO(game.get('pgn')))\n",
        "    currentBoard = currentPgn.board()\n",
        "  except:\n",
        "    print('Something went wrong in processing game')\n",
        "    continue\n",
        "  for move in currentPgn.mainline_moves():\n",
        "      currentBoard.push(move)\n",
        "  moves = currentPgn.end().ply()\n",
        "  # get wdl for each move\n",
        "  for node in currentPgn.mainline():\n",
        "    board3d = split_dims(node.board())\n",
        "    if node.eval() != None:\n",
        "      wdl = node.eval().wdl(ply=node.ply()).relative.expectation()\n",
        "    else:\n",
        "      wdl = 0.0\n",
        "    allboards.append(board3d)\n",
        "    allevals.append(wdl)\n",
        "\n",
        "\n",
        "numpy.savez('data.npz', features=numpy.array(allboards), labels=numpy.array(allevals))\n",
        "#all evals should be an array of all evals of all positions in all games\n",
        "# so it should be an array of shape(games, positions, evals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXL1geKP7J0z",
        "outputId": "e11912ee-1944-4824-9692-e528cdeaa681"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chess in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "2373\n",
            "Something went wrong in processing game\n",
            "Something went wrong in processing game\n",
            "Something went wrong in processing game\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(allboards[0])"
      ],
      "metadata": {
        "id": "g4QOnFhMgkPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8b019e-2336-4f3b-afbe-52f457a04d3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 1 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 0 1 1 1]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 1 0 0 0 0 1 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 1 0 0 1 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [1 0 0 0 0 0 0 1]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 1 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 1 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 1 0 0 0 0 1 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 1 0 0 1 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[1 0 0 0 0 0 0 1]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 1 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [1 0 0 0 0 0 0 0]\n",
            "  [0 1 0 0 1 0 0 1]\n",
            "  [1 1 1 1 0 1 1 1]\n",
            "  [1 1 1 1 0 1 1 1]\n",
            "  [0 0 0 0 1 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1]\n",
            "  [1 1 1 1 1 1 1 1]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.models as models\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.utils as utils\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "import tensorflow.keras.callbacks as callbacks\n",
        "import numpy as np\n",
        "\n",
        "# adjust model based on the above data adjustments ^^\n",
        "def build_model():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(layers.Conv1D(filters=10, kernel_size=1, activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(pool_size=2, strides=None))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(1,activation = 'sigmoid'))\n",
        "\n",
        "  return model\n",
        "def build_model_residual(conv_size, conv_depth):\n",
        "  # adding the convolutional layers\n",
        "  x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', input_shape=(14, 8, 8))\n",
        "  for _ in range(conv_depth):\n",
        "    previous = x\n",
        "    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Add()([x, previous])\n",
        "    x = layers.Activation('relu')(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(1, 'sigmoid')(x)\n",
        "\n",
        "  return models.Model(outputs=x)\n",
        "\n",
        "\n",
        "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    if shuffle:\n",
        "        # Specify seed to always have the same split distribution between runs\n",
        "        rng = np.random.default_rng(12)\n",
        "        rng.shuffle(ds)\n",
        "        #ds = ds.shuffle(shuffle_size, seed=12)\n",
        "    train_size = int(train_split * int(ds_size))\n",
        "    val_size = int(val_split * int(ds_size))\n",
        "    \n",
        "    train_ds = ds[:train_size]  \n",
        "    val_ds = ds[train_size:val_size]\n",
        "    test_ds = ds[val_size:]\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "DDJjHPgTD0Bi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.python.ops.gen_array_ops import shape\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import callbacks, optimizers\n",
        "from tensorflow.keras.layers import (LSTM, BatchNormalization, Dense, Dropout, Flatten,\n",
        "                          TimeDistributed, Conv2D, MaxPooling2D)\n",
        "from tensorflow.keras.models import Sequential, load_model, model_from_json\n",
        "\n",
        "#So my understanding is that we need to turn our y_data into the same shape as our x_data, with everything normalized\n",
        "# So the question is, do we need to change our board representation or our eval representation?\n",
        "#turn our boards array into a numpy array and pass it into the partition function\n",
        "with np.load('/content/data.npz') as data:\n",
        "  train_examples = data['features']\n",
        "  train_labels = data['labels']\n",
        "batch_size = train_examples.size\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n",
        "print(train_dataset)\n",
        "# eventually we need to add validation for accuracy purposes\n",
        "# for better accuracy and strength increase this\n",
        "#split training\n",
        "train_examples, val_examples, test_examples = get_dataset_partitions_tf(train_examples, batch_size)\n",
        "train_labels, val_labels, test_labels = get_dataset_partitions_tf(train_labels, batch_size)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=10, kernel_size=1, activation='relu', input_shape=(14,8,8)))\n",
        "model.add(MaxPooling2D(pool_size=2, strides=None))\n",
        "model.add(Flatten())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1,activation = 'sigmoid'))\n",
        "model.compile(optimizer=optimizers.Adam(5e-4), loss=tf.keras.losses.MeanSquaredError(), metrics=['accuracy'])\n",
        "model.summary()\n",
        "checkpoint_filepath = '/tmp/checkpoint/'\n",
        "checkpoint = callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                           monitor='val_accuracy',\n",
        "                                           verbose=0,\n",
        "                                           save_best_only=True,\n",
        "                                           save_weights_only=True,\n",
        "                                           mode='auto')\n",
        "model.fit(train_examples, train_labels,\n",
        "          epochs=1000,\n",
        "          verbose=1,\n",
        "          validation_data=(val_examples, val_labels),\n",
        "          callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
        "                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4),checkpoint])\n",
        "#then after we fit it, we can evaluate it using our test examples and labels\n",
        "model.evaluate(x=test_examples, y=test_labels)\n",
        "model.save('model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GWCOG2FLParI",
        "outputId": "a69fd6e9-8216-4fad-e99b-5c54f4efd067"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TensorSliceDataset element_spec=(TensorSpec(shape=(14, 8, 8), dtype=tf.int8, name=None), TensorSpec(shape=(), dtype=tf.float64, name=None))>\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_79 (Conv2D)          (None, 14, 8, 10)         90        \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 7, 4, 10)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_19 (Flatten)        (None, 280)               0         \n",
            "                                                                 \n",
            " batch_normalization_68 (Bat  (None, 280)              1120      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1)                 281       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,491\n",
            "Trainable params: 931\n",
            "Non-trainable params: 560\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "6267/6267 [==============================] - 17s 3ms/step - loss: 0.1108 - lr: 5.0000e-04\n",
            "Epoch 2/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1069 - lr: 5.0000e-04\n",
            "Epoch 3/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1067 - lr: 5.0000e-04\n",
            "Epoch 4/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1065 - lr: 5.0000e-04\n",
            "Epoch 5/1000\n",
            "6267/6267 [==============================] - 17s 3ms/step - loss: 0.1064 - lr: 5.0000e-04\n",
            "Epoch 6/1000\n",
            "6267/6267 [==============================] - 20s 3ms/step - loss: 0.1063 - lr: 5.0000e-04\n",
            "Epoch 7/1000\n",
            "6267/6267 [==============================] - 17s 3ms/step - loss: 0.1063 - lr: 5.0000e-04\n",
            "Epoch 8/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1062 - lr: 5.0000e-04\n",
            "Epoch 9/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1062 - lr: 5.0000e-04\n",
            "Epoch 10/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1061 - lr: 5.0000e-04\n",
            "Epoch 11/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1061 - lr: 5.0000e-04\n",
            "Epoch 12/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1061 - lr: 5.0000e-04\n",
            "Epoch 13/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1060 - lr: 5.0000e-04\n",
            "Epoch 14/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1060 - lr: 5.0000e-04\n",
            "Epoch 15/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1060 - lr: 5.0000e-04\n",
            "Epoch 16/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1060 - lr: 5.0000e-04\n",
            "Epoch 17/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1060 - lr: 5.0000e-04\n",
            "Epoch 18/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 19/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 20/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 21/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 22/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 23/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 24/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 25/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 26/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 27/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 28/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 29/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1059 - lr: 5.0000e-04\n",
            "Epoch 30/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1057 - lr: 5.0000e-05\n",
            "Epoch 31/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1057 - lr: 5.0000e-05\n",
            "Epoch 32/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1057 - lr: 5.0000e-05\n",
            "Epoch 33/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1057 - lr: 5.0000e-05\n",
            "Epoch 34/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1057 - lr: 5.0000e-05\n",
            "Epoch 35/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1057 - lr: 5.0000e-05\n",
            "Epoch 36/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1057 - lr: 5.0000e-05\n",
            "Epoch 37/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1057 - lr: 5.0000e-05\n",
            "Epoch 38/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1057 - lr: 5.0000e-05\n",
            "Epoch 39/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1057 - lr: 5.0000e-05\n",
            "Epoch 40/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1057 - lr: 5.0000e-05\n",
            "Epoch 41/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1057 - lr: 5.0000e-06\n",
            "Epoch 42/1000\n",
            "6267/6267 [==============================] - 16s 2ms/step - loss: 0.1057 - lr: 5.0000e-06\n",
            "Epoch 43/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1057 - lr: 5.0000e-06\n",
            "Epoch 44/1000\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.1057 - lr: 5.0000e-06\n",
            "Epoch 45/1000\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.1057 - lr: 5.0000e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:923: RuntimeWarning: divide by zero encountered in log10\n",
            "  numdigits = int(np.log10(self.target)) + 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OverflowError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-a2c4084c2871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m                      callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4),checkpoint])\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#then after we fit it, we can evaluate it using our test examples and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0mnumdigits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'%'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumdigits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'd/%d ['\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mprog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chess Neural Network:\n",
        "\n",
        "My attempt at learning how neural networks work in regards to chess.\n",
        "\n",
        "\n",
        "Steps: \n",
        "\n",
        "\n",
        "\n",
        "1.   Get games from lichess (by month, or fetch all games of the top 100 users of rapid) \n",
        "2.   convert format if needed\n",
        "\n",
        "\n",
        "#TODO\n",
        "\n",
        "1. Run model with multiple games\n",
        "2. Learn more about how to design tf models to better improve loss\n",
        "3. connect to gpu and use gpu processing ftw\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aGi4nsiltHWy"
      }
    }
  ]
}